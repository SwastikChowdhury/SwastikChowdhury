## 🚀 About Me
👋 Hi, I’m **Swastik Chowdhury** — a Data Engineer & Machine Learning Engineer with 5+ years of experience leading large-scale data and AI initiatives! <br>
🎓 Graduate Student @ Carnegie Mellon University<br>
🚀 Actively seeking full-time roles starting August 2025<br<br>
📍 **Location:** Pittsburgh, PA, USA <br>
💼 **Open to:** Data Engineering, Machine Learning Engineering, Software Engineering roles<br>
📝 **Blog:** [https://medium.com/@swastiksc1996](https://medium.com/@swastiksc1996)<br>
🔗 **LinkedIn:** [linkedin.com/in/swastik-chowdhury](https://linkedin.com/in/swastik-chowdhury)<br>

---

## 🛠️ Technical Skills
💻 **Languages:** Python, Scala, Java, SQL<br>
🗄️ **Databases:** Oracle SQL, MongoDB, Cassandra, Redis, Neo4j<br>
🛢️ **Big Data:** Spark, PySpark, Kafka, Hadoop<br>
📊 **Analytics:** Hypothesis Testing, A/B Testing, Survival Analysis, ML, DL, GenAI, NLP<br>
🏢 **Data Warehousing:** Redshift, BigQuery, Iceberg, Hive, Trino<br>
⚙️ **DevOps/MLOps:** Docker, Airflow, dbt, RabbitMQ, Jenkins, Ansible, ArgoCD, Kubernetes, Istio, Prometheus, Grafana, Git, Bash, Postman, Zeno, MLflow<br>
☁️ **Cloud:** AWS, GCP<br>
📈 **BI/Visualization:** Tableau, QuickSight<br>

---

## 🏆 Key Achievements
**Cost Optimization:** Led migration from Amazon Redshift to EMR using Apache Spark, saving $1M+ annually<br>
**Real-Time Data Ingestion:** Designed CDC pipelines with Debezium, Kafka, and Iceberg for scalable, real-time analytics<br>
**Spark Optimization:** Reduced Spark job costs by 15%+ through advanced resource allocation and workload analysis<br>
**AI Product Development:** Built a production-ready tool for document indexing using LlamaIndex and LLMs, streamlining support and knowledge sharing<br>
**Teaching:** Graduate Teaching Assistant for Machine Learning in Production/AI Engineering at CMU, mentoring teams on deploying ML systems in production<br>

---
